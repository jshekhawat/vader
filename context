Repo structure:
  Cargo.lock
  Cargo.toml
  README.md
  context
  examples/record.rs
  examples/segment.rs
  examples/wav.rs
  examples/whisper/Cargo.lock
  examples/whisper/Cargo.toml
  examples/whisper/src/main.rs
  examples/whisper/src/whisper.rs
  src/helpers.rs
  src/lib.rs
  src/session.rs
  src/vad.rs
  src/vad_result.rs


--- File: Cargo.toml ---

[workspace]
members = ["examples/whisper"]

[package]
name = "vad-rs"
version = "0.1.5"
edition = "2021"
description = "Speech detection using silero vad in Rust"
license = "MIT"

[dependencies]
eyre = "0.6.12"
ndarray = "0.16.1"
ort = "=2.0.0-rc.9"
samplerate = { git = "https://github.com/jshekhawat/rust-samplerate.git", optional = true }
ebur128 = { version = "0.1.9", optional = true }
ringbuffer = "0.15.0"

[dev-dependencies]
hound = { version = "3.5.1" }
cpal = "0.15.3"
once_cell = "1.19.0"


[features]
default = ["helpers"]
helpers = ["dep:ebur128", "dep:samplerate"]
coreml = ["ort/coreml"]
directml = ["ort/directml"]
load-dynamic = ["ort/load-dynamic"]


--- File: README.md ---

# vad-rs

Speech detection using silero vad in Rust

# Install

```console
cargo add vad-rs
```

# Examples

See [examples](examples)


--- File: context ---



--- File: examples/record.rs ---

/*
wget https://github.com/thewh1teagle/vad-rs/releases/download/v0.1.0/silero_vad.onnx
cargo run --example record silero_vad.onnx
*/

use cpal::traits::{DeviceTrait, HostTrait, StreamTrait};
use cpal::Sample;
use eyre::{bail, Result};
use once_cell::sync::Lazy;
use ringbuffer::{AllocRingBuffer, RingBuffer};
use std::sync::atomic::{AtomicBool, AtomicUsize, Ordering};
use std::sync::{Arc, Mutex};
use std::time::Instant;
use vad_rs::{Normalizer, Vad, VadStatus};

// Options
static MIN_SPEECH_DUR: Lazy<usize> = Lazy::new(|| 700); // 0.6s
static MIN_SILENCE_DUR: Lazy<usize> = Lazy::new(|| 700);

// Vad
static VAD_BUF: Lazy<Mutex<AllocRingBuffer<f32>>> =
    Lazy::new(|| Mutex::new(AllocRingBuffer::new(16000)));

// State
static IS_SPEECH: Lazy<Arc<AtomicBool>> = Lazy::new(|| Arc::new(AtomicBool::new(false)));
static SPEECH_DUR: Lazy<Arc<AtomicUsize>> = Lazy::new(|| Arc::new(AtomicUsize::new(0)));
static SILENCE_DUR: Lazy<Arc<AtomicUsize>> = Lazy::new(|| Arc::new(AtomicUsize::new(0)));
static NORMALIZER: Lazy<Arc<Mutex<Option<Normalizer>>>> = Lazy::new(|| Arc::new(None.into()));

fn build_stream<T>(
    device: &cpal::Device,
    config: &cpal::StreamConfig,
    sample_rate: u32,
    channels: u16,
    vad_handle: Arc<Mutex<Vad>>,
) -> Result<cpal::Stream>
where
    T: Sample + cpal::SizedSample,
{
    let err_fn = move |err| {
        eprintln!("an error occurred on stream: {}", err);
    };
    Ok(device.build_input_stream(
        config,
        move |data: &[T], _: &_| {
            on_stream_data::<T>(data, sample_rate, channels, vad_handle.clone());
        },
        err_fn,
        None,
    )?)
}

fn main() -> Result<()> {
    let vad_model_path = std::env::args()
        .nth(1)
        .expect("Please specify vad model filename");

    let vad = Vad::new(vad_model_path, 16000).unwrap();
    let vad_handle = Arc::new(Mutex::new(vad));

    let host = cpal::default_host();

    // Set up the input device and stream with the default input config.
    let device = host
        .default_input_device()
        .expect("failed to find input device");

    println!("Input device: {}", device.name()?);

    let config = device
        .default_input_config()
        .expect("Failed to get default input config");
    println!("Default input config: {:?}", config);

    // A flag to indicate that recording is in progress.
    println!("Begin recording...");

    let sample_rate = config.sample_rate().0;
    let channels = config.channels();

    // Loudness normalization

    let normalizer = Normalizer::new(config.channels().into(), config.sample_rate().0);
    *NORMALIZER.lock().unwrap() = Some(normalizer);

    let stream = match config.sample_format() {
        cpal::SampleFormat::I8 => build_stream::<i8>(
            &device,
            &config.into(),
            sample_rate,
            channels,
            vad_handle.clone(),
        )?,
        cpal::SampleFormat::I16 => build_stream::<i16>(
            &device,
            &config.into(),
            sample_rate,
            channels,
            vad_handle.clone(),
        )?,
        cpal::SampleFormat::I32 => build_stream::<i32>(
            &device,
            &config.into(),
            sample_rate,
            channels,
            vad_handle.clone(),
        )?,
        cpal::SampleFormat::F32 => build_stream::<f32>(
            &device,
            &config.into(),
            sample_rate,
            channels,
            vad_handle.clone(),
        )?,
        sample_format => {
            bail!("Unsupported sample format '{sample_format}'")
        }
    };

    stream.play()?;

    // Keep main thread alive
    loop {
        std::thread::sleep(std::time::Duration::from_secs(1));
    }
}

fn on_stream_data<T>(input: &[T], sample_rate: u32, channels: u16, vad_handle: Arc<Mutex<Vad>>)
where
    T: Sample,
{
    // Convert the input samples to f32
    let samples: Vec<f32> = input
        .iter()
        .map(|s| s.to_float_sample().to_sample())
        .collect();

    // Resample the stereo audio to the desired sample rate
    let mut resampled: Vec<f32> = vad_rs::audio_resample(&samples, sample_rate, 16000, channels);

    #[allow(unused)]
    if channels > 1 {
        resampled = vad_rs::stereo_to_mono(&resampled).unwrap();
    }

    let mut normalizer = NORMALIZER.lock().unwrap();
    let normalizer = normalizer.as_mut().unwrap();
    resampled = normalizer.normalize_loudness(&samples);

    let chunk_size = (30 * sample_rate / 1000) as usize;
    let mut vad = vad_handle.lock().unwrap();

    let mut vad_buf = VAD_BUF.lock().unwrap();
    vad_buf.extend(resampled.clone());

    if vad_buf.len() as f32 > sample_rate as f32 * 0.1 {
        // 0.1s
        // Start timing
        let start_time = Instant::now();

        // println!("compute {:?}", vad_buf.len());
        if let Ok(mut result) = vad.compute(&vad_buf.to_vec()) {
            // Calculate the elapsed time
            let elapsed_time = start_time.elapsed();
            let elapsed_ms = elapsed_time.as_secs_f64() * 1000.0;

            // Log or handle the situation if computation time exceeds a threshold
            if elapsed_ms > 100.0 {
                eprintln!(
                    "Warning: VAD computation took too long: {} ms (expected < 30 ms)",
                    elapsed_ms
                );
            }

            match result.status() {
                VadStatus::Speech => {
                    SPEECH_DUR.fetch_add(chunk_size, Ordering::Relaxed);
                    if SPEECH_DUR.load(Ordering::Relaxed) >= *MIN_SPEECH_DUR
                        && !IS_SPEECH.load(Ordering::Relaxed)
                    {
                        println!("Speech Start");
                        SILENCE_DUR.store(0, Ordering::Relaxed);
                        IS_SPEECH.store(true, Ordering::Relaxed);
                        vad_buf.extend(resampled.clone());
                    }
                }
                VadStatus::Silence => {
                    SILENCE_DUR.fetch_add(chunk_size, Ordering::Relaxed);
                    if SILENCE_DUR.load(Ordering::Relaxed) >= *MIN_SILENCE_DUR
                        && IS_SPEECH.load(Ordering::Relaxed)
                    {
                        println!("Speech End");

                        SPEECH_DUR.store(0, Ordering::Relaxed);
                        IS_SPEECH.store(false, Ordering::Relaxed);
                    }
                }
                _ => {}
            }
        }
        vad_buf.clear();
    }
}


--- File: examples/segment.rs ---

/*
wget https://github.com/thewh1teagle/vad-rs/releases/download/v0.1.0/silero_vad.onnx
wget https://github.com/thewh1teagle/vad-rs/releases/download/v0.1.0/motivation.wav
cargo run --example segment silero_vad.onnx motivation.wav
*/

use hound::WavReader;
use vad_rs::{Vad, VadStatus};

fn main() {
    let model_path = std::env::args()
        .nth(1)
        .expect("Please specify model filename");
    let audio_path = std::env::args()
        .nth(2)
        .expect("Please specify audio filename");

    let mut reader = WavReader::open(audio_path).unwrap();
    let spec = reader.spec();
    let mut vad = Vad::new(model_path, spec.sample_rate.try_into().unwrap()).unwrap();

    let chunk_size = (0.1 * spec.sample_rate as f32) as usize; // 0.1s
    let mut samples: Vec<f32> = reader
        .samples::<i16>()
        .map(|s| s.unwrap() as f32 / i16::MAX as f32)
        .collect();

    let mut is_speech = false;
    let mut start_time = 0.0;
    let mut speech_duration = 0.0;
    let mut silence_duration = 0.0;
    let sample_rate = spec.sample_rate as f32;

    let min_speech_dur = 0.3; // Minimum speech duration in seconds
    let min_silence_dur = 0.5; // Minimum silence duration in seconds

    // Add 1s of silence to the end of the samples
    samples.extend(vec![0.0; sample_rate as usize]);

    for (i, chunk) in samples.chunks(chunk_size).enumerate() {
        let time = i as f32 * chunk_size as f32 / sample_rate;
        match vad.compute(chunk) {
            Ok(mut result) => match result.status() {
                VadStatus::Speech => {
                    if is_speech {
                        speech_duration += chunk_size as f32 / sample_rate;
                    } else {
                        if silence_duration >= min_silence_dur {
                            silence_duration = 0.0;
                        }
                        start_time = time;
                        speech_duration = chunk_size as f32 / sample_rate;
                        is_speech = true;
                    }
                }
                VadStatus::Silence => {
                    if is_speech {
                        if speech_duration >= min_speech_dur {
                            println!("Speech detected from {:.2}s to {:.2}s", start_time, time);
                        }
                        silence_duration = chunk_size as f32 / sample_rate;
                        speech_duration = 0.0;
                        is_speech = false;
                    } else {
                        silence_duration += chunk_size as f32 / sample_rate;
                    }
                }
                _ => {}
            },
            Err(error) => {
                eprintln!("error: {:?}", error);
            }
        }
    }
}


--- File: examples/wav.rs ---

/*
wget https://github.com/thewh1teagle/vad-rs/releases/download/v0.1.0/silero_vad.onnx
wget https://github.com/thewh1teagle/vad-rs/releases/download/v0.1.0/motivation.wav
cargo run --example wav silero_vad.onnx motivation.wav
*/

use hound::WavReader;
use vad_rs::{Vad, VadStatus};

fn main() {
    let model_path = std::env::args()
        .nth(1)
        .expect("Please specify model filename");
    let audio_path = std::env::args()
        .nth(2)
        .expect("Please specify audio filename");

    let mut reader = WavReader::open(audio_path).unwrap();
    let spec = reader.spec();
    let mut vad = Vad::new(model_path, spec.sample_rate.try_into().unwrap()).unwrap();

    let chunk_size = (0.1 * spec.sample_rate as f32) as usize; // 0.1s
    let mut samples: Vec<f32> = reader
        .samples::<i16>()
        .map(|s| s.unwrap() as f32 / i16::MAX as f32)
        .collect();

    let mut is_speech = false;
    let mut start_time = 0.0;
    let sample_rate = spec.sample_rate as f32;

    // Add 1s of silence to the end of the samples
    samples.extend(vec![0.0; sample_rate as usize]);

    for (i, chunk) in samples.chunks(chunk_size).enumerate() {
        let time = i as f32 * chunk_size as f32 / sample_rate;

        if let Ok(mut result) = vad.compute(chunk) {
            match result.status() {
                VadStatus::Speech => {
                    if !is_speech {
                        start_time = time;
                        is_speech = true;
                    }
                }
                VadStatus::Silence => {
                    if is_speech {
                        println!("Speech detected from {:.2}s to {:.2}s", start_time, time);
                        is_speech = false;
                    }
                }
                _ => {}
            }
        }
    }
}


--- File: examples/whisper/Cargo.toml ---

[package]
name = "whisper"
version = "0.1.0"
edition = "2021"

[dependencies]

vad-rs = { path = "../../" }
cpal = "0.15.3"
samplerate = "0.2.4"
eyre = "0.6.12"
once_cell = "1.19.0"
ringbuffer = "0.15.0"


[target.'cfg(target_os = "macos")'.dependencies]
whisper-rs = { git = "https://github.com/thewh1teagle/whisper-rs.git", branch = "v1.6.3-beta.0", features = [
    "whisper-cpp-tracing",
    "coreml",
] }

[target.'cfg(not(target_os = "macos"))'.dependencies]
whisper-rs = { git = "https://github.com/thewh1teagle/whisper-rs.git", branch = "v1.6.3-beta.0", features = [
    "whisper-cpp-tracing",
    "vulkan",
] }


--- File: examples/whisper/src/main.rs ---

/*
wget https://github.com/thewh1teagle/vad-rs/releases/download/v0.1.0/silero_vad.onnx
wget https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-small.bin
cargo run silero_vad.onnx ggml-small.bin

Note: In Windows install Vulkan SDK from https://vulkan.lunarg.com and set VULKAN_SDK = "C:\VulkanSDK\<version>"
Note: In Linux install Vulkan SDK from https://vulkan.lunarg.com and also 'mesa-vulkan-drivers libvulkan1' packages
*/

use cpal::traits::{DeviceTrait, HostTrait, StreamTrait};
use cpal::Sample;
use eyre::{bail, Result};
use once_cell::sync::Lazy;
use ringbuffer::{AllocRingBuffer, RingBuffer};
use std::sync::atomic::{AtomicBool, AtomicUsize, Ordering};
use std::sync::{Arc, Mutex};
use std::time::Instant;
use vad_rs::{Normalizer, Vad, VadStatus};
mod whisper;

// Options
static MIN_SPEECH_DUR: Lazy<usize> = Lazy::new(|| 700); // 0.6s
static MIN_SILENCE_DUR: Lazy<usize> = Lazy::new(|| 1500);

// Vad
static VAD_BUF: Lazy<Mutex<AllocRingBuffer<f32>>> =
    Lazy::new(|| Mutex::new(AllocRingBuffer::new(16000))); // 1s
static PRE_SPEECH_BUF: Lazy<Mutex<AllocRingBuffer<f32>>> =
    Lazy::new(|| Mutex::new(AllocRingBuffer::new(16000 * 2))); // 2s
static NORMALIZER: Lazy<Arc<Mutex<Option<Normalizer>>>> = Lazy::new(|| Arc::new(None.into()));
static SPEECH_BUF: Lazy<Mutex<AllocRingBuffer<f32>>> =
    Lazy::new(|| Mutex::new(AllocRingBuffer::new(16000 * 30))); // 30s

// State
static IS_SPEECH: Lazy<Arc<AtomicBool>> = Lazy::new(|| Arc::new(AtomicBool::new(false)));
static SPEECH_DUR: Lazy<Arc<AtomicUsize>> = Lazy::new(|| Arc::new(AtomicUsize::new(0)));
static SILENCE_DUR: Lazy<Arc<AtomicUsize>> = Lazy::new(|| Arc::new(AtomicUsize::new(0)));

fn build_stream<T>(
    device: &cpal::Device,
    config: &cpal::StreamConfig,
    sample_rate: u32,
    channels: u16,
    vad_handle: Arc<Mutex<Vad>>,
) -> Result<cpal::Stream>
where
    T: Sample + cpal::SizedSample,
{
    let err_fn = move |err| {
        eprintln!("an error occurred on stream: {}", err);
    };
    Ok(device.build_input_stream(
        config,
        move |data: &[T], _: &_| {
            on_stream_data::<T>(data, sample_rate, channels, vad_handle.clone());
        },
        err_fn,
        None,
    )?)
}

fn main() -> Result<()> {
    let vad_model_path = std::env::args()
        .nth(1)
        .expect("Please specify vad model filename");
    let whisper_model_path = std::env::args()
        .nth(2)
        .expect("Please specify whisper model filename");

    let vad = Vad::new(vad_model_path, 16000).unwrap();
    let vad_handle = Arc::new(Mutex::new(vad));

    let normalizer = Normalizer::new(1, 16000);
    *NORMALIZER.lock().unwrap() = Some(normalizer);

    let host = cpal::default_host();

    // Set up the input device and stream with the default input config.
    let device = host
        .default_input_device()
        .expect("failed to find input device");

    println!("Input device: {}", device.name()?);

    let config = device
        .default_input_config()
        .expect("Failed to get default input config");
    println!("Default input config: {:?}", config);

    // A flag to indicate that recording is in progress.
    println!("Begin recording...");

    let sample_rate = config.sample_rate().0;
    let channels = config.channels();

    // Whisper
    whisper::init(&whisper_model_path);

    let stream = match config.sample_format() {
        cpal::SampleFormat::I8 => build_stream::<i8>(
            &device,
            &config.into(),
            sample_rate,
            channels,
            vad_handle.clone(),
        )?,
        cpal::SampleFormat::I16 => build_stream::<i16>(
            &device,
            &config.into(),
            sample_rate,
            channels,
            vad_handle.clone(),
        )?,
        cpal::SampleFormat::I32 => build_stream::<i32>(
            &device,
            &config.into(),
            sample_rate,
            channels,
            vad_handle.clone(),
        )?,
        cpal::SampleFormat::F32 => build_stream::<f32>(
            &device,
            &config.into(),
            sample_rate,
            channels,
            vad_handle.clone(),
        )?,
        sample_format => {
            bail!("Unsupported sample format '{sample_format}'")
        }
    };

    stream.play()?;

    // Keep main thread alive
    loop {
        std::thread::sleep(std::time::Duration::from_secs(1));
    }
}

fn transcribe_in_background() {
    std::thread::spawn(|| {
        let mut speech_buf = SPEECH_BUF.lock().unwrap();
        let samples = speech_buf.to_vec();

        let min_samples = (1.0 * 16_000.0) as usize;
        if samples.len() < min_samples {
            println!("Less than 1s. Skipping...");
            return;
        }

        if let Some(text) = whisper::transcribe(&samples) {
            println!("text: {}", text);
        }
        speech_buf.clear();
    });
}

fn on_stream_data<T>(input: &[T], sample_rate: u32, channels: u16, vad_handle: Arc<Mutex<Vad>>)
where
    T: Sample,
{
    // Convert the input samples to f32
    let samples: Vec<f32> = input
        .iter()
        .map(|s| s.to_float_sample().to_sample())
        .collect();

    // Resample the stereo audio to the desired sample rate
    let mut resampled: Vec<f32> = vad_rs::audio_resample(&samples, sample_rate, 16000, channels);

    if channels > 1 {
        resampled = vad_rs::stereo_to_mono(&resampled).unwrap();
    }
    // Normalize loudness
    let mut normalizer = NORMALIZER.lock().unwrap();
    let normalizer = normalizer.as_mut().unwrap();
    let resampled = normalizer.normalize_loudness(&resampled);

    let chunk_size = (30 * sample_rate / 1000) as usize;
    let mut vad = vad_handle.lock().unwrap();

    let mut vad_buf = VAD_BUF.lock().unwrap();
    let mut pre_speech_buf = PRE_SPEECH_BUF.lock().unwrap();
    vad_buf.extend(resampled.clone());

    // Store pre speech samples (2s)
    pre_speech_buf.extend(resampled.clone());
    // push speech samples to speech buf
    if IS_SPEECH.load(Ordering::Relaxed) {
        SPEECH_BUF.lock().unwrap().extend(resampled.clone());
    }

    // 1s audio in vad buffer
    if vad_buf.len() as f32 > sample_rate as f32 * 0.1 {
        // 0.1s
        // Start timing
        let start_time = Instant::now();

        // println!("compute {:?}", vad_buf.len());
        if let Ok(mut result) = vad.compute(&vad_buf.to_vec()) {
            // Calculate the elapsed time
            let elapsed_time = start_time.elapsed();
            let elapsed_ms = elapsed_time.as_secs_f64() * 1000.0;

            // Log or handle the situation if computation time exceeds a threshold
            if elapsed_ms > 100.0 {
                eprintln!(
                    "Warning: VAD computation took too long: {} ms (expected < 30 ms)",
                    elapsed_ms
                );
            }

            match result.status() {
                VadStatus::Speech => {
                    SPEECH_DUR.fetch_add(chunk_size, Ordering::Relaxed);
                    if SPEECH_DUR.load(Ordering::Relaxed) >= *MIN_SPEECH_DUR
                        && !IS_SPEECH.load(Ordering::Relaxed)
                    {
                        println!("Speech Start");
                        SILENCE_DUR.store(0, Ordering::Relaxed);
                        IS_SPEECH.store(true, Ordering::Relaxed);
                        let mut speech_buf = SPEECH_BUF.lock().unwrap();
                        speech_buf.extend(pre_speech_buf.to_vec());
                        speech_buf.extend(resampled.clone());
                    }
                }
                VadStatus::Silence => {
                    SILENCE_DUR.fetch_add(chunk_size, Ordering::Relaxed);
                    if SILENCE_DUR.load(Ordering::Relaxed) >= *MIN_SILENCE_DUR
                        && IS_SPEECH.load(Ordering::Relaxed)
                    {
                        println!("Speech End");

                        transcribe_in_background();

                        SPEECH_DUR.store(0, Ordering::Relaxed);
                        IS_SPEECH.store(false, Ordering::Relaxed);
                    }
                }
                _ => {}
            }
        }
        vad_buf.clear();
    }
}


--- File: examples/whisper/src/whisper.rs ---

use std::sync::{Arc, Mutex};

use once_cell::sync::Lazy;
use whisper_rs::{
    FullParams, SamplingStrategy, WhisperContext, WhisperContextParameters, WhisperState,
};

static WHISPER_STATE: Lazy<Arc<Mutex<Option<WhisperState>>>> =
    Lazy::new(|| Arc::new(Mutex::new(None)));
static WHISPER_PARAMS: Lazy<Mutex<Option<FullParams>>> = Lazy::new(|| Mutex::new(None));

pub fn init(model_path: &str) {
    // Whisper
    let ctx =
        WhisperContext::new_with_params(model_path, WhisperContextParameters::default()).unwrap();
    let state = ctx.create_state().expect("failed to create key");
    whisper_rs::install_whisper_tracing_trampoline();
    let mut params = FullParams::new(SamplingStrategy::default());

    params.set_print_progress(false);
    params.set_print_realtime(false);
    params.set_print_special(false);
    params.set_print_timestamps(false);
    params.set_language(Some("en"));

    *WHISPER_STATE.lock().unwrap() = Some(state);
    *WHISPER_PARAMS.lock().unwrap() = Some(params);
}

pub fn transcribe(samples: &[f32]) -> Option<String> {
    let min_samples = (1.0 * 16_000.0) as usize;
    if samples.len() < min_samples {
        println!("Less than 1s. Skipping...");
        return None;
    }
    let state = WHISPER_STATE.clone();
    let mut state = state.lock().unwrap();
    let state = state.as_mut().unwrap();
    let params = WHISPER_PARAMS.lock().unwrap();
    let mut params = params.clone().unwrap();

    params.set_print_progress(false);
    params.set_print_realtime(false);
    params.set_print_special(false);
    params.set_print_timestamps(false);
    params.set_language(Some("en"));

    state.full(params, samples).unwrap();
    let text = state.full_get_segment_text_lossy(0).unwrap();
    Some(text)
}


--- File: src/helpers.rs ---

use ebur128::EbuR128;
use eyre::{bail, Result};

pub fn audio_resample(
    data: &[f32],
    sample_rate0: u32,
    sample_rate: u32,
    channels: u16,
) -> Vec<f32> {
    use samplerate::{convert, ConverterType};
    convert(
        sample_rate0 as _,
        sample_rate as _,
        channels as _,
        ConverterType::SincBestQuality,
        data,
    )
    .unwrap_or_default()
}

pub fn stereo_to_mono(stereo_data: &[f32]) -> Result<Vec<f32>> {
    if stereo_data.len() % 2 != 0 {
        bail!("Stereo data length should be even.")
    }

    let mut mono_data = Vec::with_capacity(stereo_data.len() / 2);

    for chunk in stereo_data.chunks_exact(2) {
        let average = (chunk[0] + chunk[1]) / 2.0;
        mono_data.push(average);
    }

    Ok(mono_data)
}

pub struct Normalizer {
    ebur128: EbuR128,
}

impl Normalizer {
    pub fn new(channels: u32, sample_rate: u32) -> Self {
        let ebur128 = ebur128::EbuR128::new(channels, sample_rate, ebur128::Mode::all())
            .expect("Failed to create ebur128");
        Self { ebur128 }
    }

    /// Normalize loudness using ebur128. making the volume stable if too quiet / loud.
    pub fn normalize_loudness(&mut self, samples: &[f32]) -> Vec<f32> {
        // Apply loudness normalization
        self.ebur128.add_frames_f32(samples).unwrap();
        let loudness = self
            .ebur128
            .loudness_global()
            .expect("Failed to get global loudness");
        let target_loudness = -23.0; // EBU R128 target loudness
        let gain = 10f32.powf(((target_loudness - loudness) / 20.0) as f32);

        // Apply gain and clamp the result
        let normalized_samples: Vec<f32> = samples
            .iter()
            .map(|&sample| (sample * gain).clamp(-1.0, 1.0))
            .collect();

        normalized_samples
    }
}


--- File: src/lib.rs ---

mod session;
mod vad;
mod vad_result;

#[cfg(feature = "helpers")]
mod helpers;

#[cfg(feature = "helpers")]
pub use helpers::{audio_resample, stereo_to_mono, Normalizer};

pub use vad::Vad;
pub use vad_result::VadStatus;


--- File: src/session.rs ---

use std::path::Path;

use eyre::Result;
use ort::session::{builder::GraphOptimizationLevel, Session};

pub fn create_session<P: AsRef<Path>>(path: P) -> Result<Session> {
    let session = Session::builder()?
        .with_optimization_level(GraphOptimizationLevel::Level3)?
        .with_intra_threads(1)?
        .with_inter_threads(1)?
        .commit_from_file(path.as_ref())?;
    Ok(session)
}


--- File: src/vad.rs ---

use eyre::{bail, Result};
use ndarray::{Array1, Array2, Array3, ArrayBase, Ix1, Ix3, OwnedRepr};
use ort::session::Session;
use std::path::Path;

use crate::{session, vad_result::VadResult};

#[derive(Debug)]
pub struct Vad {
    session: Session,
    h_tensor: ArrayBase<OwnedRepr<f32>, Ix3>,
    c_tensor: ArrayBase<OwnedRepr<f32>, Ix3>,
    sample_rate_tensor: ArrayBase<OwnedRepr<i64>, Ix1>,
}

impl Vad {
    pub fn new<P: AsRef<Path>>(model_path: P, sample_rate: usize) -> Result<Self> {
        if ![8000_usize, 16000].contains(&sample_rate) {
            bail!("Unsupported sample rate, use 8000 or 16000!");
        }
        let session = session::create_session(model_path)?;
        let h_tensor = Array3::<f32>::zeros((2, 1, 64));
        let c_tensor = Array3::<f32>::zeros((2, 1, 64));
        let sample_rate_tensor = Array1::from_vec(vec![sample_rate as i64]);

        Ok(Self {
            session,
            h_tensor,
            c_tensor,
            sample_rate_tensor,
        })
    }

    pub fn compute(&mut self, samples: &[f32]) -> Result<VadResult> {
        let samples_tensor = Array2::from_shape_vec((1, samples.len()), samples.to_vec())?;
        let result = self.session.run(ort::inputs![
            "input" => samples_tensor.view(),
            "sr" => self.sample_rate_tensor.view(),
            "h" => self.h_tensor.view(),
            "c" => self.c_tensor.view()
        ]?)?;

        // Update internal state tensors.
        self.h_tensor = result
            .get("hn")
            .unwrap()
            .try_extract_tensor::<f32>()
            .unwrap()
            .to_owned()
            .into_shape_with_order((2, 1, 64))
            .expect("Shape mismatch for h_tensor");
        self.c_tensor = result
            .get("cn")
            .unwrap()
            .try_extract_tensor::<f32>()
            .unwrap()
            .to_owned()
            .into_shape_with_order((2, 1, 64))
            .expect("Shape mismatch for h_tensor");

        let prob = *result
            .get("output")
            .unwrap()
            .try_extract_tensor::<f32>()
            .unwrap()
            .first()
            .unwrap();
        Ok(VadResult { prob })
    }

    pub fn reset(&mut self) {
        self.h_tensor.fill(0.0);
        self.c_tensor.fill(0.0);
    }
}


--- File: src/vad_result.rs ---

#[derive(Debug, PartialEq)]
pub enum VadStatus {
    Speech,
    Silence,
    Unknown,
}
pub struct VadResult {
    pub prob: f32,
}

// https://github.com/WenqingZong/Silero_VAD/blob/main/src/lib.rs
impl VadResult {
    pub fn status(&mut self) -> VadStatus {
        if self.prob > 0.5 {
            return VadStatus::Speech;
        }
        if self.prob < 0.35 {
            return VadStatus::Silence;
        }
        VadStatus::Unknown
    }
}

